# Extract and save only the best performing Random Forest model from merged dataset

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
import joblib

# Reload and prepare data
df = pd.read_csv("/mnt/data/MERGED_ALL.csv")
target = 'price(in lakhs)'
X = df.drop(columns=[target])
y = df[target]

# Drop high-cardinality columns
drop_cols = [col for col in X.columns if 'model_' in col or 'variantName_' in col or 'oem_' in col]
X = X.drop(columns=drop_cols)

# Impute and scale
imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)

scaler = MinMaxScaler()
y_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).ravel()

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_imputed, y_scaled, test_size=0.2, random_state=42)

# Train Random Forest
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Save model, imputer, scaler
joblib.dump(rf_model, "/mnt/data/random_forest_model.pkl")
joblib.dump(imputer, "/mnt/data/imputer.pkl")
joblib.dump(MinMaxScaler().fit(X), "/mnt/data/scaler.pkl")  # For input normalization, use fitted X scaler

"/mnt/data/random_forest_model.pkl", "/mnt/data/imputer.pkl", "/mnt/data/scaler.pkl"
