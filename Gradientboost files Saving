import pickle
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split

# Load dataset
data = pd.read_csv(r"C:\Users\rkpha\Desktop\cardekho\app\segmented_structured_dataset.csv")

# Preprocessing
data = data.drop(columns=['Unnamed: 9'], errors='ignore')  # Drop unnamed if exists
X = data.drop(columns=['Prize'])
y = data['Prize']

# Setup imputer and scaler
imputer = SimpleImputer(strategy='median')
scaler = MinMaxScaler()

# Impute and scale
X_imputed = imputer.fit_transform(X.select_dtypes(include=['int64', 'float64']))
X_scaled = scaler.fit_transform(X_imputed)

# Prepare final X (including one-hot encoding for categorical features)
X_encoded = pd.get_dummies(X, drop_first=True)

# Save feature names
feature_names = X_encoded.columns.tolist()

# Train model
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)
model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
model.fit(X_train, y_train)

# Save all necessary files
with open('Gradientboost_model.pkl', 'wb') as f:
    pickle.dump(model, f)

with open('imputer.pkl', 'wb') as f:
    pickle.dump(imputer, f)

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

with open('feature_names.pkl', 'wb') as f:
    pickle.dump(feature_names, f)

# Save price range
price_min = y.min()
price_max = y.max()
np.save('price_range.npy', np.array([price_min, price_max]))
