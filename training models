import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load the preprocessed dataset
dataset = pd.read_csv(r"C:\Users\rkpha\Desktop\cardekho\app\processed_data.csv")  # Adjust the path accordingly

# Separate features and target
X = dataset.drop(columns=['Prize'])
y = dataset['Prize']

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the models
models = {
    'Linear Regression': LinearRegression(),
    'Decision Tree Regressor': DecisionTreeRegressor(random_state=42),
    'Random Forest Regressor': RandomForestRegressor(random_state=42),
    'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42),
    'XGBoost Regressor': XGBRegressor(random_state=42)
}

# Define hyperparameter grids for tuning (for each model)
param_grids = {
    'Linear Regression': {},
    'Decision Tree Regressor': {
        'max_depth': [5, 10, 15],
        'min_samples_split': [2, 5, 10]
    },
    'Random Forest Regressor': {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 15],
        'min_samples_split': [2, 5, 10]
    },
    'Gradient Boosting Regressor': {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7]
    },
    'XGBoost Regressor': {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7]
    }
}


# Function to train and evaluate each model
def train_and_evaluate(models, param_grids, X_train, X_test, y_train, y_test):
    results = []

    for model_name, model in models.items():
        print(f"Training {model_name}...")

        # Hyperparameter tuning using GridSearchCV
        grid_search = GridSearchCV(model, param_grids[model_name], cv=5, n_jobs=-1, scoring='neg_mean_squared_error')
        grid_search.fit(X_train, y_train)

        # Best model and parameters
        best_model = grid_search.best_estimator_
        best_params = grid_search.best_params_

        # Model prediction
        y_pred = best_model.predict(X_test)

        # Evaluation metrics
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        results.append({
            "Model": model_name,
            "MAE": round(mae, 4),
            "MSE": round(mse, 4),
            "R2 Score": round(r2, 4)
        })


        results_df = pd.DataFrame(results)
        print("\nðŸ”µ Model Performance Comparison:\n")
        print(results_df.sort_values(by="R2 Score", ascending=False))


# Train and evaluate the models
results_df = train_and_evaluate(models, param_grids, X_train, X_test, y_train, y_test)

# Display the results
print(results_df)


